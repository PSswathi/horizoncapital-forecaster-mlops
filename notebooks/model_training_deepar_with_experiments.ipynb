{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c3755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Bucket: nfci-forecasting-222634372778\n",
      "Shapes: (6800, 80) (1100, 80) (1100, 80)\n"
     ]
    }
   ],
   "source": [
    "# Setup + Load splits\n",
    "import os, sys, json\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from time import gmtime, strftime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from config.config import BUCKET_NAME, S3_PREFIX\n",
    "\n",
    "load_dotenv()\n",
    "ROLE_ARN = os.getenv(\"SAGEMAKER_ROLE_ARN\")\n",
    "if not ROLE_ARN:\n",
    "    raise RuntimeError(\"SAGEMAKER_ROLE_ARN not found in .env\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "print(\"Region:\", region)\n",
    "print(\"Bucket:\", BUCKET_NAME)\n",
    "\n",
    "train_s3 = f\"s3://{BUCKET_NAME}/{S3_PREFIX['train']}/features.parquet\"\n",
    "val_s3   = f\"s3://{BUCKET_NAME}/{S3_PREFIX['validation']}/features.parquet\"\n",
    "test_s3  = f\"s3://{BUCKET_NAME}/{S3_PREFIX['test']}/features.parquet\"\n",
    "\n",
    "df_train = pd.read_parquet(train_s3)\n",
    "df_val   = pd.read_parquet(val_s3)\n",
    "df_test  = pd.read_parquet(test_s3)\n",
    "\n",
    "for df in (df_train, df_val, df_test):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "print(\"Shapes:\", df_train.shape, df_val.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df144322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 50 series -> train.jsonl\n",
      "Wrote 0 series -> validation.jsonl\n",
      "Wrote 0 series -> test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Convert to DeepAR JSONL (1 line per state series)\n",
    "ITEM_COL = \"state_fips\"\n",
    "TIME_COL = \"date\"\n",
    "TARGET_COL = \"NFCI\"\n",
    "\n",
    "def to_deepar_jsonl(df: pd.DataFrame, out_path: str):\n",
    "    df = df[[ITEM_COL, TIME_COL, TARGET_COL]].copy()\n",
    "    df = df.sort_values([ITEM_COL, TIME_COL]).reset_index(drop=True)\n",
    "\n",
    "    n_series = 0\n",
    "    with open(out_path, \"w\") as f:\n",
    "        for item_id, g in df.groupby(ITEM_COL):\n",
    "            g = g.sort_values(TIME_COL)\n",
    "\n",
    "            start = g[TIME_COL].iloc[0].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            target = g[TARGET_COL].astype(float).tolist()\n",
    "\n",
    "            # keep only states with enough history\n",
    "            if len(target) < 24:\n",
    "                continue\n",
    "\n",
    "            f.write(json.dumps({\"start\": start, \"target\": target}) + \"\\n\")\n",
    "            n_series += 1\n",
    "\n",
    "    print(f\"Wrote {n_series} series -> {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "train_jsonl = to_deepar_jsonl(df_train, \"train.jsonl\")\n",
    "val_jsonl   = to_deepar_jsonl(df_val, \"validation.jsonl\")\n",
    "test_jsonl  = to_deepar_jsonl(df_test, \"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06f93fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.jsonl lines: 50\n",
      "validation.jsonl lines: 0\n",
      "test.jsonl lines: 0\n"
     ]
    }
   ],
   "source": [
    "def count_lines(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "print(\"train.jsonl lines:\", count_lines(\"train.jsonl\"))\n",
    "print(\"validation.jsonl lines:\", count_lines(\"validation.jsonl\"))\n",
    "print(\"test.jsonl lines:\", count_lines(\"test.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26fbdbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: s3://sagemaker-us-east-1-222634372778/nfci-deepar-1step/train/train.jsonl\n",
      "val:   s3://sagemaker-us-east-1-222634372778/nfci-deepar-1step/validation/validation.jsonl\n",
      "test:  s3://sagemaker-us-east-1-222634372778/nfci-deepar-1step/test/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Upload JSONL to S3\n",
    "DEEPar_PREFIX = \"nfci-deepar-1step\"\n",
    "\n",
    "train_s3_uri = sess.upload_data(\"train.jsonl\", key_prefix=f\"{DEEPar_PREFIX}/train\")\n",
    "val_s3_uri   = sess.upload_data(\"validation.jsonl\", key_prefix=f\"{DEEPar_PREFIX}/validation\")\n",
    "test_s3_uri  = sess.upload_data(\"test.jsonl\", key_prefix=f\"{DEEPar_PREFIX}/test\")\n",
    "\n",
    "print(\"train:\", train_s3_uri)\n",
    "print(\"val:  \", val_s3_uri)\n",
    "print(\"test: \", test_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36c971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: nfci-deepar-1step-exp\n",
      "Run: run-20260205-100933\n"
     ]
    }
   ],
   "source": [
    "# SageMaker Experiments: create/load an Experiment + create a Trial for this run\n",
    "from sagemaker.experiments.experiment import Experiment\n",
    "from sagemaker.experiments.run import Run\n",
    "\n",
    "EXPERIMENT_NAME = \"nfci-deepar-1step-exp\"\n",
    "\n",
    "# Create or load Experiment (idempotent)\n",
    "try:\n",
    "    exp = Experiment.load(\n",
    "        experiment_name=EXPERIMENT_NAME,\n",
    "        sagemaker_session=sess\n",
    "    )\n",
    "except Exception:\n",
    "    exp = Experiment.create(\n",
    "        experiment_name=EXPERIMENT_NAME,\n",
    "        description=\"DeepAR 1-step NFCI experiments\",\n",
    "        sagemaker_session=sess,\n",
    "    )\n",
    "\n",
    "run_name = f\"run-{strftime('%Y%m%d-%H%M%S', gmtime())}\"\n",
    "\n",
    "print(\"Experiment:\", EXPERIMENT_NAME)\n",
    "print(\"Run:\", run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf1395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: deepar-1step-2026-02-05-10-29-47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-05 10:27:09 Starting - Starting the training job...\n",
      "2026-02-05 10:27:40 Starting - Preparing the instances for training...\n",
      "2026-02-05 10:28:01 Downloading - Downloading input data...\n",
      "2026-02-05 10:28:22 Downloading - Downloading the training image.........\n",
      "2026-02-05 10:30:03 Training - Training image download completed. Training in progress.Docker entrypoint called with argument(s): train\n",
      "Running default environment configuration script\n",
      "Running custom environment configuration script\n",
      "/opt/amazon/lib/python3.9/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\n",
      "[02/05/2026 10:30:12 INFO 139752390506304] Reading default configuration from /opt/amazon/lib/python3.9/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\n",
      "[02/05/2026 10:30:12 INFO 139752390506304] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '6', 'dropout_rate': '0.1', 'epochs': '15', 'learning_rate': '0.001', 'likelihood': 'gaussian', 'mini_batch_size': '64', 'num_cells': '40', 'num_layers': '2', 'prediction_length': '1', 'time_freq': 'M'}\n",
      "[02/05/2026 10:30:12 INFO 139752390506304] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.1', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'gaussian', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '6', 'epochs': '15', 'prediction_length': '1', 'time_freq': 'M'}\n",
      "Process 7 is a worker.\n",
      "[02/05/2026 10:30:12 INFO 139752390506304] Detected entry point for worker worker\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] random_seed is None\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.jsonl` and will NOT be used for training.\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.jsonl` and will NOT be used for training.\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Training set statistics:\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Real time series\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] number of time series: 50\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] number of observations: 6800\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] mean target length: 136.0\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] min/mean/max target: -0.7947499752044678/-0.2139410411610323/3.0533699989318848\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] mean abs(target): 0.6117676566628849\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] contains missing values: no\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Small number of time series. Doing 13 passes over dataset with prob 0.9846153846153847 per epoch.\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] No test channel found not running evaluations\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #memory_usage::<batchbuffer> = 0.537109375 mb\n",
      "/opt/amazon/python3.9/lib/python3.9/subprocess.py:941: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] nvidia-smi: took 0.039 seconds to run.\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] nvidia-smi identified 0 GPUs.\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Number of GPUs being used: 0\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Create Store: local\n",
      "#metrics {\"StartTime\": 1770287413.1100411, \"EndTime\": 1770287413.125358, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 14.36924934387207, \"count\": 1, \"min\": 14.36924934387207, \"max\": 14.36924934387207}}}\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Number of GPUs being used: 0\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #memory_usage::<model> = 3 mb\n",
      "#metrics {\"StartTime\": 1770287413.125427, \"EndTime\": 1770287413.146803, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 36.6671085357666, \"count\": 1, \"min\": 36.6671085357666, \"max\": 36.6671085357666}}}\n",
      "[10:30:13] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.485.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 10240 bytes with malloc directly\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Epoch[0] Batch[0] avg_epoch_loss=0.735126\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=0.7351257801055908\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Epoch[0] Batch[5] avg_epoch_loss=0.651338\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=0.6513377527395884\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Epoch[0] Batch [5]#011Speed: 3583.09 samples/sec#011loss=0.651338\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] processed a total of 601 examples\n",
      "#metrics {\"StartTime\": 1770287413.146853, \"EndTime\": 1770287413.4961941, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"update.time\": {\"sum\": 349.2543697357178, \"count\": 1, \"min\": 349.2543697357178, \"max\": 349.2543697357178}}}\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1720.3073651510333 records/second\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #progress_metric: host=algo-1, completed 6.666666666666667 % of epochs\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #quality_metric: host=algo-1, epoch=0, train loss <loss>=0.5379274696111679\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Epoch[1] Batch[0] avg_epoch_loss=0.450343\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=0.4503430426120758\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Epoch[1] Batch[5] avg_epoch_loss=0.231969\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=0.23196949064731598\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Epoch[1] Batch [5]#011Speed: 3760.95 samples/sec#011loss=0.231969\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Epoch[1] Batch[10] avg_epoch_loss=0.158560\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=0.07046838644891977\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Epoch[1] Batch [10]#011Speed: 3594.13 samples/sec#011loss=0.070468\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] processed a total of 661 examples\n",
      "#metrics {\"StartTime\": 1770287413.496261, \"EndTime\": 1770287413.8230138, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 326.28917694091797, \"count\": 1, \"min\": 326.28917694091797, \"max\": 326.28917694091797}}}\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=2025.0705186639552 records/second\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #progress_metric: host=algo-1, completed 13.333333333333334 % of epochs\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #quality_metric: host=algo-1, epoch=1, train loss <loss>=0.15855989782986316\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] Epoch[2] Batch[0] avg_epoch_loss=0.007620\n",
      "[02/05/2026 10:30:13 INFO 139752390506304] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=0.007620316930115223\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[2] Batch[5] avg_epoch_loss=-0.073294\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=-0.0732942153699696\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[2] Batch [5]#011Speed: 3444.96 samples/sec#011loss=-0.073294\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] processed a total of 609 examples\n",
      "#metrics {\"StartTime\": 1770287413.8231041, \"EndTime\": 1770287414.1343808, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 310.99510192871094, \"count\": 1, \"min\": 310.99510192871094, \"max\": 310.99510192871094}}}\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1957.668894880524 records/second\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #progress_metric: host=algo-1, completed 20.0 % of epochs\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #quality_metric: host=algo-1, epoch=2, train loss <loss>=-0.13619058756157756\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[3] Batch[0] avg_epoch_loss=-0.246867\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=-0.24686670303344727\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[3] Batch[5] avg_epoch_loss=-0.225499\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=-0.22549874087174734\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[3] Batch [5]#011Speed: 3581.36 samples/sec#011loss=-0.225499\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[3] Batch[10] avg_epoch_loss=-0.301098\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=-0.39181687235832213\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[3] Batch [10]#011Speed: 3666.10 samples/sec#011loss=-0.391817\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] processed a total of 647 examples\n",
      "#metrics {\"StartTime\": 1770287414.1344392, \"EndTime\": 1770287414.4599273, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 325.0892162322998, \"count\": 1, \"min\": 325.0892162322998, \"max\": 325.0892162322998}}}\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1989.4743892389936 records/second\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #progress_metric: host=algo-1, completed 26.666666666666668 % of epochs\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #quality_metric: host=algo-1, epoch=3, train loss <loss>=-0.30109789154746314\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[4] Batch[0] avg_epoch_loss=-0.451456\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=-0.4514555037021637\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[4] Batch[5] avg_epoch_loss=-0.483548\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=-0.48354751368363696\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[4] Batch [5]#011Speed: 3775.61 samples/sec#011loss=-0.483548\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[4] Batch[10] avg_epoch_loss=-0.503941\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=-0.5284128069877625\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[4] Batch [10]#011Speed: 3608.85 samples/sec#011loss=-0.528413\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] processed a total of 655 examples\n",
      "#metrics {\"StartTime\": 1770287414.4600194, \"EndTime\": 1770287414.7827687, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 322.3719596862793, \"count\": 1, \"min\": 322.3719596862793, \"max\": 322.3719596862793}}}\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=2031.18650483829 records/second\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #progress_metric: host=algo-1, completed 33.333333333333336 % of epochs\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #quality_metric: host=algo-1, epoch=4, train loss <loss>=-0.5039408288218759\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] Epoch[5] Batch[0] avg_epoch_loss=-0.449135\n",
      "[02/05/2026 10:30:14 INFO 139752390506304] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=-0.44913455843925476\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[5] Batch[5] avg_epoch_loss=-0.544329\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=-0.5443288485209147\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[5] Batch [5]#011Speed: 3288.47 samples/sec#011loss=-0.544329\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[5] Batch[10] avg_epoch_loss=-0.481630\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=-0.4063913643360138\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[5] Batch [10]#011Speed: 3175.08 samples/sec#011loss=-0.406391\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] processed a total of 644 examples\n",
      "#metrics {\"StartTime\": 1770287414.7828398, \"EndTime\": 1770287415.1280553, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 344.80905532836914, \"count\": 1, \"min\": 344.80905532836914, \"max\": 344.80905532836914}}}\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1867.1774893390818 records/second\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #progress_metric: host=algo-1, completed 40.0 % of epochs\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #quality_metric: host=algo-1, epoch=5, train loss <loss>=-0.4816299920732325\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[6] Batch[0] avg_epoch_loss=-0.522944\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=-0.5229437351226807\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[6] Batch[5] avg_epoch_loss=-0.686806\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=-0.6868059237798055\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[6] Batch [5]#011Speed: 2542.46 samples/sec#011loss=-0.686806\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] processed a total of 614 examples\n",
      "#metrics {\"StartTime\": 1770287415.128119, \"EndTime\": 1770287415.5211608, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 392.6725387573242, \"count\": 1, \"min\": 392.6725387573242, \"max\": 392.6725387573242}}}\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1562.95584000369 records/second\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #progress_metric: host=algo-1, completed 46.666666666666664 % of epochs\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #quality_metric: host=algo-1, epoch=6, train loss <loss>=-0.6285788178443908\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[7] Batch[0] avg_epoch_loss=-0.628191\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=-0.6281911730766296\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[7] Batch[5] avg_epoch_loss=-0.731171\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=-0.7311711510022482\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[7] Batch [5]#011Speed: 3144.33 samples/sec#011loss=-0.731171\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[7] Batch[10] avg_epoch_loss=-0.703147\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=-0.6695190310478211\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] Epoch[7] Batch [10]#011Speed: 2784.79 samples/sec#011loss=-0.669519\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] processed a total of 688 examples\n",
      "#metrics {\"StartTime\": 1770287415.5212805, \"EndTime\": 1770287415.8952153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 373.54469299316406, \"count\": 1, \"min\": 373.54469299316406, \"max\": 373.54469299316406}}}\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1841.3196382549304 records/second\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #progress_metric: host=algo-1, completed 53.333333333333336 % of epochs\n",
      "[02/05/2026 10:30:15 INFO 139752390506304] #quality_metric: host=algo-1, epoch=7, train loss <loss>=-0.7031474601138722\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] Epoch[8] Batch[0] avg_epoch_loss=-0.545825\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=-0.5458248853683472\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] Epoch[8] Batch[5] avg_epoch_loss=-0.682739\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=-0.6827388207117716\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] Epoch[8] Batch [5]#011Speed: 2386.98 samples/sec#011loss=-0.682739\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] processed a total of 579 examples\n",
      "#metrics {\"StartTime\": 1770287415.8952706, \"EndTime\": 1770287416.306634, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 410.7182025909424, \"count\": 1, \"min\": 410.7182025909424, \"max\": 410.7182025909424}}}\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1408.51150645937 records/second\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] #progress_metric: host=algo-1, completed 60.0 % of epochs\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] #quality_metric: host=algo-1, epoch=8, train loss <loss>=-0.77498379945755\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] Epoch[9] Batch[0] avg_epoch_loss=-0.505782\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=-0.5057821273803711\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] Epoch[9] Batch[5] avg_epoch_loss=-0.638303\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=-0.6383033394813538\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] Epoch[9] Batch [5]#011Speed: 2668.94 samples/sec#011loss=-0.638303\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] processed a total of 634 examples\n",
      "#metrics {\"StartTime\": 1770287416.3069494, \"EndTime\": 1770287416.7472548, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 439.4049644470215, \"count\": 1, \"min\": 439.4049644470215, \"max\": 439.4049644470215}}}\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1442.388421355814 records/second\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] #progress_metric: host=algo-1, completed 66.66666666666667 % of epochs\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] #quality_metric: host=algo-1, epoch=9, train loss <loss>=-0.6307824671268463\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] Epoch[10] Batch[0] avg_epoch_loss=-0.999327\n",
      "[02/05/2026 10:30:16 INFO 139752390506304] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=-0.9993267059326172\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] Epoch[10] Batch[5] avg_epoch_loss=-0.773658\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=-0.7736584941546122\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] Epoch[10] Batch [5]#011Speed: 2925.61 samples/sec#011loss=-0.773658\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] processed a total of 622 examples\n",
      "#metrics {\"StartTime\": 1770287416.747363, \"EndTime\": 1770287417.1044793, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 356.5499782562256, \"count\": 1, \"min\": 356.5499782562256, \"max\": 356.5499782562256}}}\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1743.36518518172 records/second\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #progress_metric: host=algo-1, completed 73.33333333333333 % of epochs\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #quality_metric: host=algo-1, epoch=10, train loss <loss>=-0.8381249725818634\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] Epoch[11] Batch[0] avg_epoch_loss=-0.879411\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=-0.8794108629226685\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] Epoch[11] Batch[5] avg_epoch_loss=-0.889375\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=-0.8893745442231497\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] Epoch[11] Batch [5]#011Speed: 3591.35 samples/sec#011loss=-0.889375\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] processed a total of 635 examples\n",
      "#metrics {\"StartTime\": 1770287417.1046393, \"EndTime\": 1770287417.4335184, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 328.27234268188477, \"count\": 1, \"min\": 328.27234268188477, \"max\": 328.27234268188477}}}\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1933.8034687285128 records/second\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #progress_metric: host=algo-1, completed 80.0 % of epochs\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #quality_metric: host=algo-1, epoch=11, train loss <loss>=-0.8639298677444458\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] Epoch[12] Batch[0] avg_epoch_loss=-0.434402\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=-0.43440186977386475\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] Epoch[12] Batch[5] avg_epoch_loss=-0.883109\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=-0.8831090728441874\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] Epoch[12] Batch [5]#011Speed: 3107.45 samples/sec#011loss=-0.883109\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] Epoch[12] Batch[10] avg_epoch_loss=-0.935503\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=-0.998375940322876\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] Epoch[12] Batch [10]#011Speed: 3434.44 samples/sec#011loss=-0.998376\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] processed a total of 643 examples\n",
      "#metrics {\"StartTime\": 1770287417.4335806, \"EndTime\": 1770287417.7793512, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 345.35694122314453, \"count\": 1, \"min\": 345.35694122314453, \"max\": 345.35694122314453}}}\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1861.2006727254281 records/second\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #progress_metric: host=algo-1, completed 86.66666666666667 % of epochs\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #quality_metric: host=algo-1, epoch=12, train loss <loss>=-0.9355031035163186\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] Epoch[13] Batch[0] avg_epoch_loss=-0.746956\n",
      "[02/05/2026 10:30:17 INFO 139752390506304] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=-0.7469556331634521\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Epoch[13] Batch[5] avg_epoch_loss=-0.972482\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=-0.9724819163481394\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Epoch[13] Batch [5]#011Speed: 3568.23 samples/sec#011loss=-0.972482\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Epoch[13] Batch[10] avg_epoch_loss=-0.983127\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=-0.9959019899368287\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Epoch[13] Batch [10]#011Speed: 3369.63 samples/sec#011loss=-0.995902\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] processed a total of 660 examples\n",
      "#metrics {\"StartTime\": 1770287417.7794385, \"EndTime\": 1770287418.1119797, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 332.25083351135254, \"count\": 1, \"min\": 332.25083351135254, \"max\": 332.25083351135254}}}\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=1985.9621896486415 records/second\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] #progress_metric: host=algo-1, completed 93.33333333333333 % of epochs\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] #quality_metric: host=algo-1, epoch=13, train loss <loss>=-0.9831274043429982\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Epoch[14] Batch[0] avg_epoch_loss=-0.957683\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=-0.9576831459999084\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Epoch[14] Batch[5] avg_epoch_loss=-0.825289\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=-0.8252886633078257\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Epoch[14] Batch [5]#011Speed: 3772.65 samples/sec#011loss=-0.825289\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] processed a total of 639 examples\n",
      "#metrics {\"StartTime\": 1770287418.1120346, \"EndTime\": 1770287418.430218, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 317.9600238800049, \"count\": 1, \"min\": 317.9600238800049, \"max\": 317.9600238800049}}}\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] #throughput_metric: host=algo-1, train throughput=2009.1081037927145 records/second\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] #progress_metric: host=algo-1, completed 100.0 % of epochs\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] #quality_metric: host=algo-1, epoch=14, train loss <loss>=-0.8727932035923004\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Final loss: -0.8727932035923004 (occurred at epoch 14)\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] #quality_metric: host=algo-1, train final_loss <loss>=-0.8727932035923004\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Worker algo-1 finished training.\n",
      "[02/05/2026 10:30:18 WARNING 139752390506304] wait_for_all_workers will not sync workers since the kv store is not running distributed\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] All workers finished. Serializing model for prediction.\n",
      "#metrics {\"StartTime\": 1770287418.4302776, \"EndTime\": 1770287418.4466317, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 15.546798706054688, \"count\": 1, \"min\": 15.546798706054688, \"max\": 15.546798706054688}}}\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Number of GPUs being used: 0\n",
      "#metrics {\"StartTime\": 1770287418.4466834, \"EndTime\": 1770287418.4589472, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 27.902841567993164, \"count\": 1, \"min\": 27.902841567993164, \"max\": 27.902841567993164}}}\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Serializing to /opt/ml/model/model_algo-1\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\n",
      "#metrics {\"StartTime\": 1770287418.4589965, \"EndTime\": 1770287418.4610226, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 1.9898414611816406, \"count\": 1, \"min\": 1.9898414611816406, \"max\": 1.9898414611816406}}}\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] Successfully serialized the model for prediction.\n",
      "[02/05/2026 10:30:18 INFO 139752390506304] No test data passed, skipping evaluation.\n",
      "#metrics {\"StartTime\": 1770287418.461062, \"EndTime\": 1770287418.4623775, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 4.4002532958984375, \"count\": 1, \"min\": 4.4002532958984375, \"max\": 4.4002532958984375}, \"totaltime\": {\"sum\": 5535.050392150879, \"count\": 1, \"min\": 5535.050392150879, \"max\": 5535.050392150879}}}\n",
      "\n",
      "2026-02-05 10:30:36 Uploading - Uploading generated training model\n",
      "2026-02-05 10:30:36 Completed - Training job completed\n",
      "Training seconds: 155\n",
      "Billable seconds: 155\n",
      "Training completed: deepar-1step-2026-02-05-10-29-47\n",
      "Logged to Experiment: nfci-deepar-1step-exp | Run: run-20260205-102947\n"
     ]
    }
   ],
   "source": [
    "# training + Experiments logging (NO test channel)\n",
    "from sagemaker.experiments.run import Run\n",
    "from time import gmtime, strftime\n",
    "\n",
    "deepar_image = sagemaker.image_uris.retrieve(\n",
    "    framework=\"forecasting-deepar\",\n",
    "    region=region,\n",
    "    version=\"1\"\n",
    ")\n",
    "\n",
    "job_name = \"deepar-1step-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "output_path = f\"s3://{BUCKET_NAME}/{DEEPar_PREFIX}/output/{job_name}\"\n",
    "\n",
    "deepar = Estimator(\n",
    "    image_uri=deepar_image,\n",
    "    role=ROLE_ARN,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# baseline (your earlier better-performing params)\n",
    "deepar.set_hyperparameters(\n",
    "    time_freq=\"M\",\n",
    "    prediction_length=1,\n",
    "    context_length=32,\n",
    "    epochs=15,\n",
    "    mini_batch_size=64,\n",
    "    learning_rate=1e-3,\n",
    "    num_cells=40,\n",
    "    num_layers=2,\n",
    "    dropout_rate=0.1,\n",
    "    likelihood=\"gaussian\",\n",
    ")\n",
    "\n",
    "train_input = TrainingInput(train_s3_uri, content_type=\"json\")\n",
    "\n",
    "run_name = f\"run-{strftime('%Y%m%d-%H%M%S', gmtime())}\"\n",
    "\n",
    "with Run(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    run_name=run_name,\n",
    "    sagemaker_session=sess,\n",
    ") as run:\n",
    "\n",
    "    # Train with ONLY train channel (no test/validation)\n",
    "    deepar.fit(\n",
    "        {\"train\": train_input},\n",
    "        job_name=job_name,\n",
    "        logs=True\n",
    "    )\n",
    "\n",
    "    # log params + pointers\n",
    "    run.log_parameters(deepar.hyperparameters())\n",
    "    run.log_parameter(\"training_job_name\", job_name)\n",
    "    run.log_parameter(\"train_s3_uri\", train_s3_uri)\n",
    "\n",
    "print(\"Training completed:\", job_name)\n",
    "print(\"Logged to Experiment:\", EXPERIMENT_NAME, \"| Run:\", run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83018f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: forecasting-deepar-2026-02-05-10-41-08-188\n",
      "INFO:sagemaker:Creating endpoint-config with name forecasting-deepar-2026-02-05-10-41-08-188\n",
      "INFO:sagemaker:Creating endpoint with name forecasting-deepar-2026-02-05-10-41-08-188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Deploy\n",
    "predictor = deepar.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    ")\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "ITEM_COL = \"state_fips\"\n",
    "TIME_COL = \"date\"\n",
    "TARGET_COL = \"NFCI\"\n",
    "\n",
    "def deepar_predict_1step(predictor, start, history):\n",
    "    hist = []\n",
    "    for x in history:\n",
    "        try:\n",
    "            fx = float(x)\n",
    "            if np.isfinite(fx):\n",
    "                hist.append(fx)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if len(hist) < 2:\n",
    "        raise ValueError(\"History too short after cleaning (need at least 2 points).\")\n",
    "\n",
    "    payload = {\n",
    "        \"instances\": [{\"start\": start, \"target\": hist}],\n",
    "        \"configuration\": {\"num_samples\": 200, \"output_types\": [\"mean\"]},\n",
    "    }\n",
    "    resp = predictor.predict(payload)\n",
    "    return float(resp[\"predictions\"][0][\"mean\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32c922c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.10072702624678334\n",
      "Test MAE:  0.07883151024336364\n",
      "Test R2:   -3.8290357501258887\n",
      "N eval points: 1100\n"
     ]
    }
   ],
   "source": [
    "# ---- Evaluation (uses df_train + df_test like your earlier working notebook) ----\n",
    "df_train_eval = df_train.copy()\n",
    "df_test_eval  = df_test.copy()\n",
    "df_train_eval[TIME_COL] = pd.to_datetime(df_train_eval[TIME_COL])\n",
    "df_test_eval[TIME_COL]  = pd.to_datetime(df_test_eval[TIME_COL])\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "states = sorted(set(df_train_eval[ITEM_COL].unique()).intersection(set(df_test_eval[ITEM_COL].unique())))\n",
    "MIN_HISTORY = 6\n",
    "\n",
    "for sid in states:\n",
    "    g_tr = df_train_eval[df_train_eval[ITEM_COL] == sid].sort_values(TIME_COL).reset_index(drop=True)\n",
    "    g_te = df_test_eval[df_test_eval[ITEM_COL] == sid].sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "    if len(g_te) == 0:\n",
    "        continue\n",
    "\n",
    "    history_clean = []\n",
    "    for x in g_tr[TARGET_COL].tolist():\n",
    "        try:\n",
    "            fx = float(x)\n",
    "            if np.isfinite(fx):\n",
    "                history_clean.append(fx)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if len(history_clean) < MIN_HISTORY:\n",
    "        continue\n",
    "\n",
    "    start = pd.to_datetime(g_tr[TIME_COL].iloc[0]).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    for actual in g_te[TARGET_COL].tolist():\n",
    "        try:\n",
    "            a = float(actual)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not np.isfinite(a):\n",
    "            continue\n",
    "\n",
    "        pred = deepar_predict_1step(predictor, start, history_clean)\n",
    "        y_true.append(a)\n",
    "        y_pred.append(pred)\n",
    "        history_clean.append(a)\n",
    "\n",
    "y_true = np.array(y_true, dtype=float)\n",
    "y_pred = np.array(y_pred, dtype=float)\n",
    "\n",
    "if len(y_true) == 0:\n",
    "    raise ValueError(\"No evaluation points collected. Check your df_train/df_test split and NFCI values.\")\n",
    "\n",
    "mse  = mean_squared_error(y_true, y_pred)\n",
    "rmse = float(np.sqrt(mse))\n",
    "mae  = float(mean_absolute_error(y_true, y_pred))\n",
    "r2   = float(r2_score(y_true, y_pred))\n",
    "\n",
    "print(\"Test RMSE:\", rmse)\n",
    "print(\"Test MAE: \", mae)\n",
    "print(\"Test R2:  \", r2)\n",
    "print(\"N eval points:\", len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f62ad98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.experiments.run:The run (run-20260205-102947) under experiment (nfci-deepar-1step-exp) already exists. Loading it.\n"
     ]
    }
   ],
   "source": [
    "# Log evaluation metrics to SageMaker Experiments (as a separate TrialComponent)\n",
    "#from sagemaker.experiments.trial_component import TrialComponent\n",
    "from sagemaker.experiments.run import Run\n",
    "\n",
    "# Log metrics (rmse/mae/r2 must already exist from Cell A)\n",
    "with Run(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    run_name=run_name,          # must match the run_name you used for training\n",
    "    sagemaker_session=sess,\n",
    ") as run:\n",
    "    run.log_metric(\"test_rmse\", rmse)\n",
    "    run.log_metric(\"test_mae\", mae)\n",
    "    run.log_metric(\"test_r2\", r2)\n",
    "    run.log_metric(\"n_eval_points\", int(len(y_true)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cdfd709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: forecasting-deepar-2026-02-05-10-41-08-188\n",
      "INFO:sagemaker:Deleting endpoint with name: forecasting-deepar-2026-02-05-10-41-08-188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged metrics to: nfci-deepar-1step-exp | Run: run-20260205-102947\n"
     ]
    }
   ],
   "source": [
    "# cleanup to avoid charges\n",
    "predictor.delete_endpoint()\n",
    "print(\"Logged metrics to:\", EXPERIMENT_NAME, \"| Run:\", run_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
