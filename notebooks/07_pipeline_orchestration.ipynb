{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 07: Pipeline Orchestration with SageMaker Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/fatimatatanda/Library/Application Support/sagemaker/config.yaml\n",
      "SageMaker SDK version: 2.256.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from config.config import (\n",
    "    BUCKET_NAME,\n",
    "    S3_PREFIX,\n",
    "    AWS_REGION,\n",
    "    TARGET_COLUMN\n",
    ")\n",
    "\n",
    "print(f\"SageMaker SDK version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name fatimat-admin to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Role:   arn:aws:iam::306617143793:role/sagemaker-execution-role\n"
     ]
    }
   ],
   "source": [
    "\n",
    "boto_session = boto3.Session(region_name=AWS_REGION)\n",
    "\n",
    "sagemaker_session = Session(boto_session=boto_session)\n",
    "pipeline_session = PipelineSession(boto_session=boto_session)\n",
    "\n",
    "sm_client = boto3.client('sagemaker', region_name=AWS_REGION)\n",
    "\n",
    "# Get execution role\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam_client = boto3.client('iam')\n",
    "    role = iam_client.get_role(RoleName='sagemaker-execution-role')['Role']['Arn']\n",
    "\n",
    "print(f\"Region: {AWS_REGION}\")\n",
    "print(f\"Role:   {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline parameters defined:\n",
      "  InstanceType:      ml.m5.large\n",
      "  PredictionLength:  12\n",
      "  RMSEThreshold:     0.3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    "    ParameterInteger,\n",
    ")\n",
    "\n",
    "# ── Tunable parameters ──\n",
    "param_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "param_prediction_length = ParameterInteger(\n",
    "    name=\"PredictionLength\",\n",
    "    default_value=12\n",
    ")\n",
    "\n",
    "param_rmse_threshold = ParameterFloat(\n",
    "    name=\"RMSEThreshold\",\n",
    "    default_value=0.30  # Register model only if RMSE ≤ this\n",
    ")\n",
    "\n",
    "print(\"Pipeline parameters defined:\")\n",
    "print(f\"  InstanceType:      {param_instance_type.default_value}\")\n",
    "print(f\"  PredictionLength:  {param_prediction_length.default_value}\")\n",
    "print(f\"  RMSEThreshold:     {param_rmse_threshold.default_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Preprocess Step\n",
    "\n",
    "This step runs `src/preprocess.py` inside an SKLearn container.\n",
    "It reads raw data from S3 and outputs DeepAR JSON files.\n",
    "\n",
    "**How data flows:**\n",
    "\n",
    "```\n",
    "S3 raw parquet  ──→  /opt/ml/processing/input/\n",
    "                         │\n",
    "                    preprocess.py\n",
    "                         │\n",
    "                    /opt/ml/processing/output/\n",
    "                         ├── train/train.json      ──→  S3 (auto)\n",
    "                         ├── test/test.json        ──→  S3 (auto)\n",
    "                         ├── inference/inference.json ──→ S3 (auto)\n",
    "                         └── actuals/actuals.csv   ──→  S3 (auto)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# Create the processor (defines the container + resources)\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version='1.2-1',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.large',\n",
    "    instance_count=1,\n",
    "    sagemaker_session=pipeline_session,  # Pipeline session — captures, doesn't run\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreprocessData step defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the Processing step\n",
    "raw_data_uri = f\"s3://{BUCKET_NAME}/data/raw_parquet/state_month_full.parquet\"\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=raw_data_uri,\n",
    "                destination=\"/opt/ml/processing/input\",\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                source=\"/opt/ml/processing/output/train\", \n",
    "                destination=f\"s3://{BUCKET_NAME}/pipeline/preprocess/train\"\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                source=\"/opt/ml/processing/output/test\",\n",
    "                destination=f\"s3://{BUCKET_NAME}/pipeline/preprocess/test\"\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"inference\",\n",
    "                source=\"/opt/ml/processing/output/inference\",\n",
    "                destination=f\"s3://{BUCKET_NAME}/pipeline/preprocess/inference\"\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"actuals\",   \n",
    "                source=\"/opt/ml/processing/output/actuals\",\n",
    "                destination=f\"s3://{BUCKET_NAME}/pipeline/preprocess/actuals\"\n",
    "            ),\n",
    "        ],\n",
    "        code=\"../src/preprocess.py\",\n",
    "        arguments=[\"--prediction-length\", \"12\"],\n",
    ")\n",
    "\n",
    "\n",
    "step_preprocess = ProcessingStep(\n",
    "    name=\"PreprocessData\", step_args=processor_args\n",
    ")\n",
    "\n",
    "print(\"PreprocessData step defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training Step\n",
    "\n",
    "This step trains DeepAR using the JSON files from the Preprocess step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepAR container: 522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1\n",
      "TrainDeepAR step defined\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "# Get DeepAR container image\n",
    "deepar_container = retrieve(\n",
    "    framework='forecasting-deepar',\n",
    "    region=AWS_REGION\n",
    ")\n",
    "print(f\"DeepAR container: {deepar_container}\")\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparameters = {\n",
    "    'time_freq': 'M',\n",
    "    'prediction_length': '12',\n",
    "    'context_length': '24',\n",
    "    'epochs': '100',\n",
    "    'early_stopping_patience': '10',\n",
    "    'num_cells': '40',\n",
    "    'num_layers': '2',\n",
    "    'likelihood': 'gaussian',\n",
    "    'mini_batch_size': '32',\n",
    "    'learning_rate': '0.001',\n",
    "    'num_dynamic_feat': '9',   # 5 national + 4 state features\n",
    "}\n",
    "\n",
    "# Create estimator \n",
    "deepar_estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=deepar_container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=param_instance_type,\n",
    "    output_path=f\"s3://{BUCKET_NAME}/{S3_PREFIX['models']}\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "# Define training step — references preprocess outputs\n",
    "# train_s3_uri = f\"s3://{BUCKET_NAME}/data/training/deepar/train/train.json\"\n",
    "# test_s3_uri = f\"s3://{BUCKET_NAME}/data/training/deepar/test/test.json\"\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"TrainDeepAR\",\n",
    "    step_args=deepar_estimator.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=step_preprocess.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                content_type=\"json\",\n",
    "            ),\n",
    "            \"test\": TrainingInput(\n",
    "                s3_data=step_preprocess.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "                content_type=\"json\",\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"TrainDeepAR step defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Creation and Batch Transform Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreateDeepARModel step defined\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.inputs import TransformInput\n",
    "\n",
    "# ── 7a. Create Model Step ──\n",
    "# Wraps the trained model artifact into a deployable SageMaker Model\n",
    "\n",
    "model = Model(\n",
    "    image_uri=deepar_container,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "    name=\"CreateDeepARModel\",\n",
    "    step_args=model.create(instance_type=\"ml.m5.large\"),\n",
    ")\n",
    "\n",
    "print(\"CreateDeepARModel step defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Batch Transform Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchTransform step defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Runs inference on the test data and saves predictions to S3\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{BUCKET_NAME}/pipeline/transform-output\",\n",
    "    accept=\"application/jsonlines\",\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"BatchTransform\",\n",
    "    step_args=transformer.transform(\n",
    "        data=step_preprocess.properties.ProcessingOutputConfig.Outputs[\"inference\"].S3Output.S3Uri,\n",
    "        content_type=\"application/jsonlines\",\n",
    "        split_type=\"Line\",         # Process one JSON line at a time\n",
    "        join_source=\"None\",        # Don't join input with output\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"BatchTransform step defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define the Evaluation Step\n",
    "\n",
    "This step runs `src/evaluate.py` which:\n",
    "1. Reads the Batch Transform predictions\n",
    "2. Reads the actual NFCI values (from preprocess step)\n",
    "3. Computes RMSE, MAE, R²\n",
    "4. Writes `evaluation.json`\n",
    "\n",
    "The **PropertyFile** tells the pipeline where to find the metrics\n",
    "so the ConditionStep can read them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluateModel step defined\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# PropertyFile: tells the pipeline where to find evaluation metrics\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",       \n",
    "    path=\"evaluation.json\",       \n",
    ")\n",
    "\n",
    "# Define the evaluation Processing step\n",
    "step_evaluate = ProcessingStep(\n",
    "    name=\"EvaluateModel\",\n",
    "    step_args=sklearn_processor.run(\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=step_transform.properties.TransformOutput.S3OutputPath,\n",
    "                destination=\"/opt/ml/processing/input/predictions\",\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=step_preprocess.properties.ProcessingOutputConfig.Outputs[\"actuals\"].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/input/actuals\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"evaluation\",\n",
    "                source=\"/opt/ml/processing/output/evaluation\",\n",
    "            ),\n",
    "        ],\n",
    "        code=\"../src/evaluate.py\",\n",
    "    ),\n",
    "    property_files=[evaluation_report],  # Makes evaluation.json readable by pipeline\n",
    ")\n",
    "\n",
    "print(\"EvaluateModel step defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define the Quality Gate and Model Registration\n",
    "\n",
    "The **ConditionStep** reads RMSE from `evaluation.json` and checks\n",
    "whether it meets our threshold. If yes, register the model. If no → stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CheckModelQuality + RegisterModel steps defined\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.model_metrics import ModelMetrics, MetricsSource\n",
    "\n",
    "#  Condition: RMSE ≤ threshold\n",
    "rmse_condition = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_evaluate.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"metrics.rmse\",\n",
    "    ),\n",
    "    right=param_rmse_threshold,\n",
    ")\n",
    "\n",
    "# Model Metrics \n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=step_evaluate.properties.ProcessingOutputConfig.Outputs[\"evaluation\"].S3Output.S3Uri,\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "#  Register Model (runs only if condition passes) ──\n",
    "step_register = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=deepar_estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=\"nfci-forecasting-models\",\n",
    "    approval_status=\"PendingManualApproval\",\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "\n",
    "# Condition Step ──\n",
    "step_condition = ConditionStep(\n",
    "    name=\"CheckModelQuality\",\n",
    "    conditions=[rmse_condition],\n",
    "    if_steps=[step_register],\n",
    "    else_steps=[],\n",
    ")\n",
    "\n",
    "print(\"CheckModelQuality + RegisterModel steps defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 10: Build and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline assembled\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"nfci-forecasting-pipeline\",\n",
    "    parameters=[\n",
    "        param_instance_type,\n",
    "        param_prediction_length,\n",
    "        param_rmse_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        step_preprocess,\n",
    "        step_train,\n",
    "        step_create_model,\n",
    "        step_transform,\n",
    "        step_evaluate,\n",
    "        step_condition,\n",
    "    ],\n",
    "    sagemaker_session=sagemaker_session,  # Regular sagemaker session for submission\n",
    ")\n",
    "\n",
    "print(\"Pipeline assembled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline created/updated in SageMaker\n"
     ]
    }
   ],
   "source": [
    "# Create or update the pipeline definition in SageMaker\n",
    "pipeline.upsert(role_arn=role)\n",
    "print(\"Pipeline created/updated in SageMaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline execution started\n",
      "  Execution ARN: arn:aws:sagemaker:us-east-1:306617143793:pipeline/nfci-forecasting-pipeline/execution/87qaih00spbr\n",
      "  Status: Executing\n"
     ]
    }
   ],
   "source": [
    "# Start the pipeline\n",
    "execution = pipeline.start()\n",
    "\n",
    "print(f\" Pipeline execution started\")\n",
    "print(f\"  Execution ARN: {execution.arn}\")\n",
    "print(f\"  Status: {execution.describe()['PipelineExecutionStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Monitor Pipeline Execution\n",
    "\n",
    "The pipeline runs asynchronously. Use these methods to check progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline finished: Succeeded\n"
     ]
    }
   ],
   "source": [
    "# Wait for the pipeline to complete (this blocks until done)\n",
    "# Remove this cell if you want to monitor manually instead\n",
    "execution.wait()\n",
    "print(f\"Pipeline finished: {execution.describe()['PipelineExecutionStatus']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step                      Status          Start Time\n",
      "-----------------------------------------------------------------\n",
      "RegisterModel-RegisterModel Succeeded       2026-02-06 20:42:00\n",
      "CheckModelQuality         Succeeded       2026-02-06 20:41:59\n",
      "EvaluateModel             Succeeded       2026-02-06 20:36:55\n",
      "BatchTransform            Succeeded       2026-02-06 20:30:16\n",
      "CreateDeepARModel-CreateModel Succeeded       2026-02-06 20:30:14\n",
      "TrainDeepAR               Succeeded       2026-02-06 20:25:21\n",
      "PreprocessData            Succeeded       2026-02-06 20:22:47\n"
     ]
    }
   ],
   "source": [
    "# View each step's status\n",
    "steps = execution.list_steps()\n",
    "\n",
    "print(f\"{'Step':<25} {'Status':<15} {'Start Time'}\")\n",
    "print(\"-\" * 65)\n",
    "for step in steps:\n",
    "    name = step['StepName']\n",
    "    status = step['StepStatus']\n",
    "    start = str(step.get('StartTime', ''))[:19]\n",
    "    print(f\"{name:<25} {status:<15} {start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality gate: Succeeded\n",
      "Condition outcome: True\n"
     ]
    }
   ],
   "source": [
    "# Check if the model was registered (condition step passed)\n",
    "for step in steps:\n",
    "    if step['StepName'] == 'CheckModelQuality':\n",
    "        print(f\"Quality gate: {step['StepStatus']}\")\n",
    "        \n",
    "        # Check the condition outcome from metadata\n",
    "        metadata = step.get('Metadata', {})\n",
    "        if 'Condition' in metadata:\n",
    "            outcome = metadata['Condition'].get('Outcome', 'Unknown')\n",
    "            print(f\"Condition outcome: {outcome}\")\n",
    "            \n",
    "    if step['StepName'] == 'RegisterModel':\n",
    "        print(f\"Model registration: {step['StepStatus']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMaker",
   "language": "python",
   "name": "sagemaker_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
